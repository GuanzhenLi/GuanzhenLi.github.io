<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MVP-Bench">
  <meta name="keywords" content="MVP-Bench, Visual Perception, vision-language, LVLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
  <title>MVP-Bench</title>
  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
  <meta name="google-site-verification" content="6lbYN1vX7A4sD8SrVniq84UEKyEUSBgxeP7d3FjuuK0" />

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="images/icon.jpg">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="shortcut icon" href="images/icon.jpg" type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  </head>

  <style>

    #main{
        position: relative;;
        width: 1200px;
    }

    .box{
        float: left;
        padding: 15px 0 0 15px;
/*        background-color: red;*/
    }

    .pic{
        width: 500px;
        padding: 10px;
        border: 1px solid #ccc;
        border-radius: 5px;
        background-color: #fff;
    }

    .pic img{
        width: 500px;
    }

  </style>



  <body>

<section class="hero">
  <div class="hero-body">
    <!-- <h1>MVP-Bench</h1> -->
    <!-- <h2>Can Large Vision-Language Models Conduct Multi-level Visual Perception like humans?</h2> -->
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MVP-Bench</h1>
          <h2 class="title is-3 publication-title">Can Large Vision-Language Models Conduct Multi-level Visual Perception like humans?</h2>
          <div class="is-size-5">
            <span class="author-block">
                Guanzhen Li,
              </span>
            <span class="author-block">
              <a href="https://yuxixie.github.io/" style="color:#008AD7;font-weight:normal;">Yuxi Xie</a>,
            </span>
            <span class="author-block">
              <a href="https://www.comp.nus.edu.sg/~kanmy/" style="color:#008AD7;font-weight:normal;">Min-Yen Kan</a>
            </span>

          </div>

          <br>
          <div>
            <span class="author-block"><b style="color:#00A4EF; font-weight:normal">&#x25B6 </b>National University of Singapore </span>
          </div>

          <!-- <br> -->
         <!--  <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#e08ba0; font-weight:normal"> <b>In CVPR2023</b> </b></span>
          </div> -->

          <!-- <br> -->
          <!-- <div class="column has-text-centered"> -->
            <!-- <div class="publication-links"> -->
              <!-- <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i> -->
                  <!-- </span> -->
                  <!-- <span>Paper</span> -->
                <!-- </a> -->
              <!-- </span> -->

              <!-- <span class="link-block"> -->
                <!-- <a href="https://github.com/GuanzhenLi/MVP-Bench" target="_blank" -->
                   <!-- class="external-link button is-normal is-rounded is-dark"> -->
                  <!-- <span class="icon"> -->
                      <!-- <i class="fab fa-github"></i> -->
                  <!-- </span> -->
                  <!-- <span>Code</span> -->
                  <!-- </a> -->
              <!-- </span> -->

             <!-- <span class="link-block">
                <a href="https://huggingface.co/datasets/GZClarence/MVP-Bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                ü§ó
                </span>
                <span>Space</span>
                </a>
            </span> -->

              <!-- <span class="link-block">
                <a id="randomLink" href="http://120.92.209.146:8081" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-play"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span> -->

<!--              <span class="link-block">-->
<!--                <a href="https://youtu.be/###" target="_blank"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                  </a>-->
<!--              </span>-->

              <!-- <span class="link-block"> -->
                <!-- <a href="https://huggingface.co/datasets/GZClarence/MVP-Bench" target="_blank" -->
                   <!-- class="external-link button is-normal is-rounded is-dark"> -->
                  <!-- <span class="icon"> -->
                    <!-- <i class="fa fa-database"></i> -->
                  <!-- </span> -->
                  <!-- <span>Dataset</span> -->
                  <!-- </a> -->
              <!-- </span> -->

             <!-- <span class="link-block">
               <a href="https://huggingface.co/openbmb/RLHF-V" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                 <span class="icon">
                   <i class="fa fa-laugh"></i>
                 </span>
                 <span>Model</span>
                 </a>
             </span> -->

            <!-- </div> -->
          <!-- </div> -->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Thanks for your interest in our work. Currently, the number of users has exceeded our expectations. We provide <strong><font color="#008AD7">alternative demo links</font></strong> here:
            <a href="https://b2517615b965687635.gradio.live" target="_blank">Demo1</a>
            <a href="https://c8de8ff74b6a6c6a9b.gradio.live" target="_blank">Demo2</a>
            <a href="https://90bc0bac96e6457e8f.gradio.live" target="_blank">Demo3</a>
            <a href="https://cd772059965a71f9e6.gradio.live" target="_blank">Demo4</a>
            <a href="https://48da7e23bcadec7551.gradio.live" target="_blank">Demo5</a>
            <a href="https://687d119023cd37e5fb.gradio.live" target="_blank">Demo6</a>
            <a href="https://0810e8582bcad31944.gradio.live" target="_blank">Demo7</a>
            <a href="https://31c7cdb7e3594e851e.gradio.live" target="_blank">Demo8</a>

            <strong><font>News</font></strong>: We now provide a pretrained MiniGPT-4 aligned with <strong><font color="#008AD7">Vicuna-7B</font></strong>! The demo GPU memory consumption now can be <strong><font color="#008AD7">as low as 12GB</font></strong>.
            <br>
            </p>
        </div>
      </div>
    </div>
</section>
 -->

<!-- <link rel="stylesheet" href="js/ft-carousel.css" />
<script src="js/jquery.min.js"></script>
<script src="js/ft-carousel.min.js"></script>
<script type="text/javascript">
  $("#carousel_1").FtCarousel();

  $("#carousel_2").FtCarousel({
    index: 1,
    auto: false
  });

  $("#carousel_3").FtCarousel({
    index: 0,
    auto: true,
    time: 3000,
    indicators: false,
    buttons: true
  });
</script> -->

<!--
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="demos/wop_2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <img src="demos/cook_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <img src="demos/fix_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
       </h2>
     </div>
     <div class="item">
      <img src="demos/rhyme_1.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</div>
</div>
</section>
 -->

<link rel="stylesheet" type="text/css" href="js/simple_style.css" />
<script type="text/javascript" src="js/simple_swiper.js"></script>


<!-- <div class="app">
  <div id="swiper-demo" class="simple-swiper-container">
    <a id="prev" class="btn btn-prev"></a>
    <a id="next" class="btn btn-next"></a>
    <div class="pagination"></div>
  </div>
</div>
<p id="index"></p>

<script type="text/javascript">
  new SimSwiper("#swiper-demo", {
    autoplay: 4000,
    duration: 300,
    easing: 'ease',
    button: {
      prev: "#prev", // ÂâçËøõÂêéÈÄÄÊåâÈíÆ
      next: "#next"
    },
    pagination: {
      el: '.pagination',
      click: true// ÊòØÂê¶ÂèØ‰ª•ÁÇπÂáª
    },
    // ËΩÆÊí≠ÂõæÊï∞ÊçÆ
    data: [{
      index: 0,
      href: '#',
      src: 'demos/wop_2.png'
    }, {
      index: 1,
      href: '#',
      src: 'demos/cook_1.png'
    }, {
      index: 2,
      href: '#',
      src: 'demos/fix_1.png'
    }, {
      index: 3,
      href: '#',
      src: 'demos/rhyme_1.png'
    }]
  });
</script> -->


<section class="section interpolation-panel">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-10">
        <br>
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified" style="font-size: large;">
          <p>
            Humans perform visual perception at multiple levels, including <b>low-level</b> object recognition and <b>high-level</b> semantic interpretation such as behavior understanding. <b>Subtle differences in low-level details</b> can lead to <b>substantial changes in high-level perception</b>. Despite significant advancements in various multimodal tasks, Large Visual Language Models (LVLMs) remain unexplored in their capabilities to conduct such <b>multi-level visual perceptions</b>. We introduce the <b>MVP-Bench</b> to investigate the <b>perception gap</b> btween <b>LVLMs</b> and <b>humans</b>.
            <ul>
              <li>
                <b style="font-family:Arial, Helvetica, sans-serif">The uniqness of MVP-Bench</b>: <span style="font-size: 95%;">MVP-Bench is the first visual-language benchmark systematically evaluating both low- and high-level visual perception of LVLMs. We construct MVP-Bench with image pairs consisting with a natural image and a corresponding manipulated image to investigate how manipulated content influences the models' perception. MVP-Bench contains 520 image pairs and 1872 questions at both perception levels.</span>
              </li>
              <li>
                <b style="font-family:Arial, Helvetica, sans-serif">Experiments</b>: <span style="font-size: 95%;">With MVP-Bench, we diagnose the visual perception of <b>10 open-source</b> and <b>2 closed-source</b> LVLMs and analyze the results by comparing their various performance <b>across two levels</b> and <b>across each image pair</b>.</span>
              </li>
              <li>
                <b style="font-family:Arial, Helvetica, sans-serif">Performance at difference perception levels</b>: <span style="font-size: 95%;">High-level perception tasks are <b>significantly more challenging</b> than low-level tasks for existing LVLMs. The <b>state-of-the-art</b> <a href="https://openai.com/index/hello-gpt-4o/">GPT-4o</a> only achieves an accuracy of <b>56%</b> on Yes/No questions, compared with <b>74%</b> in low-level scenarios.</span>
              </li>
              <li>
                <b style="font-family:Arial, Helvetica, sans-serif">Comparison between {natural, manipulated} images</b>: <span style="font-size: 95%;">LVLMs generally perform <b>better on natural images</b> compared to manipulated images. The performance gap indicates that current LVLMs <b>do not generalize</b> in understanding the visual semantics of synthetic images as humans do.</span>
              </li>
            </ul>
<!--             <br>
            The
            </b> -->
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- <img id="model" width="100%" src="images/rlhf-v-main_exp.jpg"> -->
       <!--  <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"></p>
        </h3> -->
    <br>
    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe width=‚Äú560‚Äù height=‚Äú315" src=‚Äúhttps://www.youtube.com/embed/__tftoxpBAw‚Äù title=‚ÄúYouTube video player‚Äù frameborder=‚Äú0‚Äù allow=‚Äúaccelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share‚Äù allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
  </div>
</section>

<!-- <section class="hero is-small is-light">
    <div class="hero-body"> -->

<!--
        <div class="container">
            <h2 class="title has-text-centered">Video Presentation</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">

                    <div class="publication-video">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/__tftoxpBAw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>
 -->


<!--     </div>
</section> -->


    <!--/ Demo. -->
    <!-- <br>
    <br>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demo</h2>
      </div>
    </div>

    <div class="column is-full-width">
      <div class="columns is-centered">
        <img id="teaser" width="90%" src="images/demo6_AdobeExpress.gif">
      </div>
      <div class="columns is-centered">
      <h1>
        <p style="font-family:Times New Roman"><b>X-GPT: Connecting generalist X-Decoder with GPT-3</b>
      </h1>
      </div>
    </div>

    <br>

    <div class="column is-full-width">
      <div class="columns is-centered">
        <img id="teaser" width="90%" src="images/inpaint.gif">
      </div>
      <div class="columns is-centered">
      <h1>
        <p style="font-family:Times New Roman"><b>Instruct-X-Decoder: Object-centric instructional image editing</b>
      </h1>
      </div>
    </div> -->

<section class="section">
  <div class="container is-max-desktop">
    <!--/ Paper video. -->
    <br>
    <br>
    <!-- Paper Model. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">An example</h2>
        <div class="content has-text-justified">
          <p>
            <b>A sample of MVP-Bench</b>:
          </p>
          <p>
            A sample of MVP-Bench manifesting both high- and low-level visual perception. <I>Image 1</I> and <I>Image 2</I> form an image pair. Their different backgrounds indicate that the man is engaged in different behaviours. In terms of the question types, we design <b>Yes/No questions</b> and <b>Multiple-choice questions</b> for MVP-Bench. Besides, to investigate whether LVLMs can discern the manipulated content across an image pair, MVP-Bench contains both <b>single-image</b> tasks and <b>cross-image</b> tasks.
          </p>
          <ul>
            <!-- <li>It has two types of queries (latent queries and text queries) and outputs (semantic outputs and pixel-level outputs).</li>
            <li>It uses a single text encoder for all text corpus, ranging from class concepts, referring phrases to image captions.</li>
            <li>It decouples image and text encoder to accomadate cross-image tasks (e.g., image-text retrieval) and within-image tasks (e.g., segmentation and captioning).</li> -->

          </ul>
        </div>
        <img id="sample" width="80%" src="static/images/Figure 1.png", alt="A sample of MVP-Bench">
       <!--  <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"></p>
        </h3> -->
        <br>
        <br>

      </div>
    </div>
    <br>
    <br>
    <!--/ Paper video. -->
        <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Definition of high- and low-level visual perception in MVP-Bench</h2>
        <br>
        <div class="content has-text-justified">
          <p>
            Perception related to <b>humans</b> normally requires more significant cognitive process <b>at both perception levels</b>, such as <I>misinformation understanding</I> and <I>emotion recognition</I>. To assess LVLMs' multi-level visual perception, MVP-Bench consists of images containing humans for ensuring that perception at <b>both levels are engaged</b> while understanding the cases.
          </p>
          <p>
            We define 5 high-level (<I>L<sub>h</sub></I>) perception categories and 13 low-level (<I>L<sub>l</sub></I>) categories. The mapping relationship between two levels indicates that certain low-level perception <b>can support</b> the high-level perception.
          </p>
        </div>
        <img id="definition" width="60%" src="static/images/Definition.png" alt="Definition of High- and Low-level visual perception in MVP-Bench.">
       <!--  <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"></p>
        </h3> -->

      </div>
    </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!--/ Paper video. -->
    <br>
    <br>
    <!-- Paper Model. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Leaderboard</h2>
        <div class="content has-text-justified">
          <p>
            <b>Definition</b>:
          </p>
          <p>
            <ul>
              <li>Single-Image: Questions querying on one of a pair of images. The questions include Yes/No questions (denoted as Yes/No) and Multiple-choice questions (denoted as MCQ).</li>
              <li>Cross-Image: Questions asking about the differences between a pair of images. These questions are Multiple-choice questions (denoted as Cross-Image), and the options are five of the defined high-level visual perception categories or low-level categories.</li>
              <li><I>L<sub>h</sub></I>, <I>L<sub>l</sub></I>, and <I>L<sub>m</sub></I>: <I>L<sub>h</sub></I> refers to the questions at high perception level; <I>L<sub>l</sub></I> refers to the questions at low perception level; <I>L<sub>m</sub></I> demonstrates LVLMs' overall mutli-level performance.</li>
              <li>M, N, and M+N: M refers to LVLMs' performance on Manipulated images; N refers to LVLMs' performance on Natural images; M+N demonstrates LVLMs' overall performance on both Natural images and Manipulated images.</li>
            </ul>
          </p>

          <p>
            <b>Metrics</b>:
          </p>
          <p>
            <ul>
              <li><I>qAcc</I>: Accuracy based on questions. With the image pair setting in MVP-Bench, we ask the same Yes/No question to a pair of images respectively. <I>qAcc</I> regards a case correct only if the model answers the question according to both images correctly.</li>
              <li><I>iAcc</I>: Accuracy based on images. In MVP-Bench, we design multiple Yes/No questions (at different levels) for each image. <I>iAcc</I> considers an image correct only if the model answers all the questions of an image correctly.</li>
              <li><I>aAcc</I>: Accuracy based on single quesion--image pair. As long as the model answers an Yes/No question of a single image correctly, <I>iAcc</I> regards this case as correct.</li>
              <li>Multiple-choice questions' evaluation: To mitigate the bias of the option order, we adopt the Circular Strategy from [MMBench](https://github.com/open-compass/MMBench) to evaluate the Single-Image and Cross-Image multiple-choice questions.</li>
              <li>We highlight the <font color="#FF0000">problematic</font> results (&it;5%) and best performance across <b>all models</b> and on <u>open-source</u> models only.</li>
            </ul>
          </p>

          <br>
          <br>

          <p>To submit your results to the leaderboard, please fill in the ‚Äúoutput‚Äù column to <a href="https://huggingface.co/datasets/GZClarence/MVP-Bench/tree/main">this leaderboard</a> jsonlines, and send to <a href="mailto:e0966232@u.nus.edu">this</a> mail.</p>
          
        </div>


        <p align="left">
          <b>Multi-level Leaderboard</b>:
        </p>
        <div class="table">
          <table border="1" width="100%">
            <thead>
              <tr class="firstHead">
                <th rowspan="3" colspan="1">Models</th>
                <th rowspan="1" colspan="7">Yes/No</th>
                <th rowspan="2" colspan="3">Cross-Image</th>
              </tr>
              <tr class="secondHead">
                <th rowspan="1" colspan="3"><I>qAcc</I></th>
                <th rowspan="1" colspan="3"><I>aAcc</I></th>
                <th rowspan="1" colspan="1"><I>mAcc</I></th>
              </tr>
              <tr class="thirdHead">
                <th rowspan="1" colspan="1"><I>L<sub>l</sub></I></th>
                <th rowspan="1" colspan="1"><I>L<sub>h</sub></I></th>
                <th rowspan="1" colspan="1"><I>L<sub>m</sub></I></th>
                <th rowspan="1" colspan="1"><I>L<sub>l</sub></I></th>
                <th rowspan="1" colspan="1"><I>L<sub>h</sub></I></th>
                <th rowspan="1" colspan="1"><I>L<sub>m</sub></I></th>
                <th rowspan="1" colspan="1"><I>L<sub>m</sub></I></th>
                <th rowspan="1" colspan="1"><I>L<sub>l</sub></I></th>
                <th rowspan="1" colspan="1"><I>L<sub>h</sub></I></th>
                <th rowspan="1" colspan="1"><I>L<sub>m</sub></I></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>DeepSeek (1.3B)</td>
                <td>63.33</td>
                <td>53.04</td>
                <td>58.60</td>
                <td>81.48</td>
                <td>75.87</td>
                <td>78.90</td>
                <td>28.40</td>
                <td>19.38</td>
                <td>18.94</td>
                <td>19.16</td>
              </tr>
              <tr>
                <td>MiniCPM-2 (3B)</td>
                <td>68.52</td>
                <td>55.22</td>
                <td>62.40</td>
                <td>84.07</td>
                <td><b><u>76.30</u></b></td>
                <td>80.50</td>
                <td>34.91</td>
                <td>29.51</td>
                <td>11.45</td>
                <td>20.48</td>
              </tr>
              <tr>
                <td>DeepSeek (7B)</td>
                <td><u>70.00</u></td>
                <td>54.35</td>
                <td><u>62.80</u></td>
                <td>84.82</td>
                <td>76.09</td>
                <td>80.00</td>
                <td>33.73</td>
                <td><u>36.12</u></td>
                <td><u>25.99</u></td>
                <td><u>31.06</u></td>
              </tr>
              <tr>
                <td>InstructBLIP (7B)</td>
                <td>49.63</td>
                <td>40.00</td>
                <td>45.20</td>
                <td>74.82</td>
                <td>69.13</td>
                <td>72.20</td>
                <td>17.75</td>
                <td><font color="#FF0000">0.00</font></td>
                <td><u>1.32</u></td>
                <td><font color="#FF0000">0.66</font></td>
              </tr>
              <tr>
                <td>LLaVA-1.5 (7B)</td>
                <td>68.89</td>
                <td>51.74</td>
                <td>61.00</td>
                <td>84.45</td>
                <td>75.44</td>
                <td>80.30</td>
                <td>31.36</td>
                <td>20.26</td>
                <td>14.10</td>
                <td>17.18</td>
              </tr>
              <tr>
                <td>MiniGPT4 (8.2B)</td>
                <td>14.44</td>
                <td>8.26</td>
                <td>11.60</td>
                <td>39.26</td>
                <td>33.70</td>
                <td>36.70</td>
                <td><font color="#FF0000">0.59</font></td>
                <td><font color="#FF0000">0.00</font></td>
                <td><font color="#FF0000">0.00</font></td>
                <td><font color="#FF0000">0.00</font></td>
              </tr>
              <tr>
                <td>MiniGPT4-v2 (8.2B)</td>
                <td>52.59</td>
                <td>40.87</td>
                <td>47.20</td>
                <td>73.70</td>
                <td>67.40</td>
                <td>70.80</td>
                <td>14.20</td>
                <td><font color="#FF0000">0.00</font></td>
                <td><font color="#FF0000">0.00</font></td>
                <td><font color="#FF0000">0.00</font></td>
              </tr>
              <tr>
                <td>mPLUG-Owl2 (8.2B)</td>
                <td>69.26</td>
                <td>54.78</td>
                <td>62.60</td>
                <td>84.63</td>
                <td><b><I>76.30</I></b></td>
                <td><u>80.80</u></td>
                <td><u>36.09</u></td>
                <td>21.14</td>
                <td>13.22</td>
                <td>17.18</td>
              </tr>
              <tr>
                <td>InstructBLIP (13B)</td>
                <td>50.37</td>
                <td>36.09</td>
                <td>43.80</td>
                <td>75.19</td>
                <td>67.61</td>
                <td>71.70</td>
                <td>15.98</td>
                <td><font color="#FF0000">1.76</font></td>
                <td><font color="#FF0000">0.44</font></td>
                <td><font color="#FF0000">1.10</font></td>
              </tr>
              <tr>
                <td>LLaVA-1.5 (13B)</td>
                <td>66.67</td>
                <td>52.17</td>
                <td>60.00</td>
                <td>83.34</td>
                <td>76.09</td>
                <td>80.00</td>
                <td>28.40</td>
                <td>25.99</td>
                <td>18.06</td>
                <td>22.03</td>
              </tr>
              <tr>
                <td>GPT-4V</td>
                <td>66.30</td>
                <td>39.57</td>
                <td>54.00</td>
                <td>82.23</td>
                <td>69.13</td>
                <td>76.20</td>
                <td>23.08</td>
                <td>44.50</td>
                <td>14.10</td>
                <td>29.30</td>
              </tr>
              <tr>
                <td>GPT-4o</td>
                <td><b>74.44</b></td>
                <td><b>56.09</b></td>
                <td><b>66.00</b></td>
                <td><b>86.85</b></td>
                <td>76.09</td>
                <td><b>81.90</b></td>
                <td><b>39.05</b></td>
                <td><b>74.01</b></td>
                <td><b>34.80</b></td>
                <td><b>54.41</b></td>
              </tr>
            </tbody>
          </table>
          <br>
          <br>
          <br>


          <p align="left">
            <b>{Natural, Manipulated} Image Pair Leaderboard</b>:
          </p>
          <div class="table">
            <table border="1" width="100%">
              <thead>
                <tr class="firstHead">
                  <th rowspan="3" colspan="1">Models</th>
                  <th rowspan="1" colspan="7">Yes/No</th>
                  <th rowspan="2" colspan="1">MCQ</th>
                </tr>
                <tr class="secondHead">
                  <th rowspan="1" colspan="3"><I>iAcc</I></th>
                  <th rowspan="1" colspan="3"><I>aAcc</I></th>
                  <th rowspan="1" colspan="1"><I>mAcc</I></th>
                </tr>
                <tr class="thirdHead">
                  <th rowspan="1" colspan="1">N</th>
                  <th rowspan="1" colspan="1">M</th>
                  <th rowspan="1" colspan="1">N+M</th>
                  <th rowspan="1" colspan="1">N</th>
                  <th rowspan="1" colspan="1">M</th>
                  <th rowspan="1" colspan="1">N+M</th>
                  <th rowspan="1" colspan="1">N+M</th>
                  <th rowspan="1" colspan="1">N+M</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>DeepSeek (1.3B)</td>
                  <td>60.95</td>
                  <td>44.38</td>
                  <td>52.66</td>
                  <td>83.20</td>
                  <td>74.60</td>
                  <td>78.90</td>
                  <td>28.40</td>
                  <td>43.78</td>
                </tr>
                <tr>
                  <td>MiniCPM-2 (3B)</td>
                  <td><u>68.64</u></td>
                  <td>53.85</td>
                  <td><u>61.24</u></td>
                  <td><u>85.20</u></td>
                  <td>75.80</td>
                  <td>80.50</td>
                  <td>34.91</td>
                  <td>44.74</td>
                </tr>
                <tr>
                  <td>DeepSeek (7B)</td>
                  <td>68.05</td>
                  <td>52.07</td>
                  <td>60.06</td>
                  <td>85.00</td>
                  <td>76.60</td>
                  <td><u>80.80</u></td>
                  <td>33.73</td>
                  <td><u>59.33</u></td>
                </tr>
                <tr>
                  <td>InstructBLIP (7B)</td>
                  <td>44.38</td>
                  <td>44.97</td>
                  <td>44.68</td>
                  <td>72.40</td>
                  <td>72.00</td>
                  <td>72.20</td>
                  <td>17.75</td>
                  <td><font color="#FF0000">4.07</font></td>
                </tr>
                <tr>
                  <td>LLaVA-1.5 (7B)</td>
                  <td>64.50</td>
                  <td>52.66</td>
                  <td>58.58</td>
                  <td>83.20</td>
                  <td>77.40</td>
                  <td>80.30</td>
                  <td>31.36</td>
                  <td>57.18</td>
                </tr>
                <tr>
                  <td>MiniGPT4 (8.2B)</td>
                  <td>10.06</td>
                  <td><font color="#FF0000">4.73</font></td>
                  <td>7.40</td>
                  <td>41.80</td>
                  <td>31.60</td>
                  <td>36.70</td>
                  <td><font color="#FF0000">0.59</font></td>
                  <td><font color="#FF0000">0.00</font></td>
                </tr>
                <tr>
                  <td>MiniGPT4-v2 (8.2B)</td>
                  <td>53.85</td>
                  <td>31.95</td>
                  <td>42.90</td>
                  <td>79.60</td>
                  <td>62.00</td>
                  <td>70.80</td>
                  <td>14.20</td>
                  <td><font color="#FF0000">1.91</font></td>
                </tr>
                <tr>
                  <td>mPLUG-Owl2 (8.2B)</td>
                  <td>66.27</td>
                  <td>54.44</td>
                  <td>60.36</td>
                  <td>84.20</td>
                  <td>77.40</td>
                  <td><u>80.80</u></td>
                  <td><u>36.09</u></td>
                  <td>50.72</td>
                </tr>
                <tr>
                  <td>InstructBLIP (13B)</td>
                  <td>41.42</td>
                  <td>46.15</td>
                  <td>43.79</td>
                  <td>70.60</td>
                  <td>72.80</td>
                  <td>71.70</td>
                  <td>15.98</td>
                  <td><font color="#FF0000">3.83</font></td>
                </tr>
                <tr>
                  <td>LLaVA-1.5 (13B)</td>
                  <td>58.58</td>
                  <td><b><u>55.62</u></b></td>
                  <td>57.10</td>
                  <td>81.20</td>
                  <td><b><u>78.80</u></b></td>
                  <td>80.00</td>
                  <td>28.40</td>
                  <td>55.02</td>
                </tr>
                <tr>
                  <td>GPT-4V</td>
                  <td>71.07</td>
                  <td>30.77</td>
                  <td>50.92</td>
                  <td>87.80</td>
                  <td>65.98</td>
                  <td>76.20</td>
                  <td>23.08</td>
                  <td>59.81</td>
                </tr>
                <tr>
                  <td>GPT-4o</td>
                  <td><b>76.92</b></td>
                  <td>48.52</td>
                  <td><b>62.72</b></td>
                  <td><b>90.00</b></td>
                  <td>73.80</td>
                  <td><b>81.90</b></td>
                  <td><b>39.05</b></td>
                  <td><b>64.83</b></td>
                </tr>
              </tbody>
            </table>
       <!--  <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"></p>
        </h3> -->
        <br>
        <br>
      </div>
    </div>
    </div>

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="columns is-centered has-text-centered">
    <h2 class="title is-3">BibTeX</h2>
    </div>
    <pre><code>
</code></pre>
  </div>
  <br>
</section>


<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Case Analysis</h2>
    </div>
  </div>
  <!--/ Results. -->
  <div class="container is-max-desktop">

    <!-- <section class="section"> -->
    <!-- <div id="main">
      <div align="center" ><div align="center" ><img src="demos/case1.png" width="75%" alt=""></div></div>
      <div align="center" ><div align="center" ><img src="demos/case2.png" width="75%" alt=""></div></div>
      <div align="center" ><div align="center" ><img src="demos/case3-1.png" width="75%" alt=""></div></div>
      <div align="center" ><div align="center" ><img src="demos/case3-2.png" width="75%" alt=""></div></div>
      <div align="center" ><div align="center" ><img src="demos/case4.png" width="75%" alt=""></div></div>
      <div align="center" ><div align="center" ><img src="demos/case5.png" width="75%" alt=""></div></div>
      <!-- <div class="box"><div class="pic"><img src="demos/p7.png" alt=""></div></div>
      <div class="box"><div class="pic"><img src="demos/p8.png" alt=""></div></div>
      <div class="box"><div class="pic"><img src="demos/p9.png" alt=""></div></div>
    </div> -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <div class="content has-text-justified">
            <p>
              <b>GPT-4V shows rigorous behaviours in High-Level Perception tasks.</b>
            </p>
            <p>GPT-4V usually approves only what it can directly observe from the image. It tends to refuse to interpret uncertain cases, such as conducting high-level perception without explicit visual clues.</p>
        </div>
        <img src="static/images/case1.png" width="90%" alt="">
        <br>
        <br>
        <br>

        <div class="content has-text-justified">
          <p>
            <b>Gaps between Open- and Closed-source LVLMs in Recognizing Visual Details and Utilizing Commonsense Knowledge.</b>
          </p>
          <p>
            Open-source LVLMs like LLaVA and DeepSeek demonstrate extraordinary performance on more straightforward cases (such as background substitution). However, they perform significantly worse on the cases requiring to recognize specific objects (like object association <I>L<sub>l</sub></I> category) or necessitating commonsense knowledge (like gesture perception).
          </p>
        </div>
        <img src="static/images/case2.png" width="90%" alt="">
        <br>
        <br>
        <br>
        
        <div class="content has-text-justified">
          <p>
            <b>Bias in LVLMs to Prioritize Dominant Components.</b>
          </p>
          <p>
            LVLMs have difficulties in comprehending an entire image (the woman's behaviour) based on an inconspicuous object (the gun). We attibute this to the data homogeneity of the LVLMs' training images.
          </p>
        </div>
        <img src="static/images/case3.png" width="50%" alt="">
        <br>
        <br>
        <br>

        <div class="content has-text-justified">
          <p>
            <b>Bias in GPT-4V and GPT-4o to Perceive Scenes as Staged Performance.</b>
          </p>
          <p>
            GPT-4V and GPT-4o tend to interpret occasional or dramatic scenes as staged images,especially when the co-occurrence frequency of visual elements is low based on commonsenseknowledge.
          </p>
        </div>
        <img src="static/images/case4.png" width="50%" alt="">
        <br>
        <br>
        <br>
        <!-- <div class="content has-text-justified">
          <ul>
            <li><b>Long-form QA</b>: RLHF-V is more resistant to over-generalization.</li>
          </ul>
        </div>
        <img src="demos/case5.png" width="90%" alt="">
      </div> -->
      <!-- <div class="box"><div class="pic"><img src="demos/p7.png" alt=""></div></div>
      <div class="box"><div class="pic"><img src="demos/p8.png" alt=""></div></div>
      <div class="box"><div class="pic"><img src="demos/p9.png" alt=""></div></div> -->
    </div>
  </div>
</section>



<!--
<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Acknowledgement</h2>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
  </div>
</section>
 -->



<script src="js/Underscore-min.js"></script>
<script src="js/index.js"></script>



</body>

</html>